{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î¶¨Î∑∞ Í∏çÏ†ï Î∂ÄÏ†ï ÌåêÎã® Î®∏Ïã†Îü¨Îãù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Ï§ÄÎπÑ\n",
    "- Í∞ÄÏÉÅÌôòÍ≤Ω ÏÖãÌåÖ\n",
    "- ÌïÑÏöî ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Îç∞Ïù¥ÌÑ∞Î°úÎìú \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [00:00<00:00, 46115.19 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [00:00<00:00, 49694.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': [\"This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\",\n",
       "  \"What a horrible movie. This movie was so out of order and so hard to follow.It was so hard to follow and was just confusing. The whole time I was watching it I was wishing it would end!!I felt like I wasted 2hours of my life that I will never get back. Save your money and don't rent this movie. I now see why Sarah Michelle Gellar was barely in the movie. The first movie was great but this was just sucked. I would never recommend this movie to anyone. Save your money and watch the trailer because that is about the only thing that is worth seeing with this movie. This movie had no real story to it either. I am still wondering what I watched.\",\n",
       "  'Sorry did i miss something? did i walk out early? The first ten minutes of unusual (and untrue!) stories had me thinking \"This is going to be a classic\" But it was all down hill from there! The acting was brilliant, for what it\\'s worth William H Macy is fantastic and just gets better and better every film i watch him in. But it never seemed to connect. I was waiting for the big moment where all the stories inter connect and then suddenly..it rains frog?? it was if the writer said \"i\\'ve gone to deep how can i pull all these stories together cleverely....Oh sod it i\\'ll just have it raining frogs\". I like clever movies, i like strange movies but this was just odd and boring. 4/10',\n",
       "  \"I loved this mini series. Tara Fitzgerald did an incredible job portraying Helen Graham, a beautiful young woman hiding, along with her young son, from a mysterious past. As an anglophile who loves romances... this movie was just my cup of tea and I would recommend it to anyone looking to escape for a few hours into the England of the 1800's. I also must mention that Toby Stephens who portrays the very magnetic Gilbert Markham is reason enough to watch this wonderful production.\",\n",
       "  'I was excited when I heard they were finally making this horrific event into a movie. The whole era (1980\\'s Southern California) and subject matter (drug and porn industry) is intriguing to me. I thought this would be a sure fire hit. I was not thrilled with the choice of Kilmer as Holmes, they do not resemble each other in physical appearance or mannerisms. I guess he sells tickets? However, I was willing to overlook this and give it a fair shot. I was a bit shocked that there were only like four other people in the entire theater with me on that first day of showing. Now the whole crime and story in the film is hard to do, I will admit that. There were no witnesses to this very violent and brutal act. John Holmes was there, but he was also a pathological liar and worried about what would happen to his family (and self) if he talked to police about it. In fact, Holmes never really testified about what happened and the crime did go unsolved. So this was still really one big mystery, a mystery that this movie does nothing to cast light on. The person writing the screenplay had a whole lot of discretion and most of the principal characters are dead. However, there is no real storyline, it is fragmented claptrap. The script is light and the actors try to hard to beef up paper thin lines by overacting. The film gives no insight into Holmes or the other people involved. Kilmer\\'s character disappears for long stretches, his girlfriend is dull, the police are jokes. Even Kudrow tries hard to make a flimsy role look substantial. It is a very shallow piece and dare I say, boring. The director even tries to turn it into a love story. Which is nice, unless you know anything about what a piece of trash John Holmes really was. Perhaps a couple of viewings of Anderson\\'s \"Boogie Nights\" might have helped here. \"Boogie Nights\" was innovative and exciting in all regards. This film on the other hand was flat and without any real charm or style. Even the music is out of place, with Duran Duran being played in a scene that was supposed to have taken place in 1980. Then we have Gordon Lightfoot? Gordon Lightfoot? There could have been a great film based on this gruesome event, but I have not seen it yet. I have not seen even a decent one yet (unless you consider the Rahad Jackson scene from Boogie Nights).'],\n",
       " 'sentiment': ['negative', 'negative', 'negative', 'positive', 'negative'],\n",
       " 'label': [0, 0, 0, 1, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hugging FaceÏóê Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎäî Î™®Îç∏ÏùÑ Î∂àÎü¨Ïò¨Í≤åÏöî. TinyBERT Î™®Îç∏ÏùÑ Î∂àÎü¨Ïò§Í≤†ÏäµÎãàÎã§.\n",
    "model = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ : This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\n",
      "ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Î≥ÄÌôòÎêú Îç∞Ïù¥ÌÑ∞ÏÖã : {'input_ids': [101, 2023, 2265, 2003, 6659, 1012, 1045, 3685, 2131, 2058, 1996, 3143, 5949, 1997, 2307, 5848, 2023, 2265, 3397, 1012, 2023, 2003, 2025, 14036, 24584, 2389, 3772, 1010, 2009, 1005, 1055, 2074, 1037, 10036, 3535, 2000, 5466, 2619, 3297, 21699, 5889, 3031, 1037, 2754, 1998, 2031, 2068, 4685, 1037, 9996, 5301, 3496, 1012, 1045, 2031, 2941, 2589, 17727, 12298, 2147, 2004, 2019, 3364, 1010, 1998, 2023, 2265, 2003, 2025, 17727, 12298, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 1996, 4378, 2003, 2941, 5870, 2012, 1006, 2065, 2027, 1005, 2128, 2941, 5870, 2012, 2023, 2265, 2012, 2035, 1010, 2009, 3504, 3243, 8275, 1007, 2003, 1996, 14325, 1997, 1996, 4113, 2732, 2108, 2439, 2066, 1037, 8448, 1999, 18167, 1012, 1996, 12873, 1010, 3294, 15142, 2477, 2027, 2272, 2039, 2007, 2024, 2054, 2111, 4756, 2012, 1012, 1998, 2065, 2009, 1005, 1055, 2025, 2112, 1997, 1996, 3496, 1010, 1996, 5889, 2097, 2425, 2068, 2008, 2009, 1005, 1055, 3308, 999, 1045, 2424, 2023, 2265, 2003, 29591, 2000, 1996, 2396, 1010, 1998, 3084, 2033, 5390, 2005, 3065, 2066, 3005, 2240, 2003, 2009, 4312, 1010, 2029, 2018, 2307, 5848, 1010, 2307, 17727, 12298, 2399, 1010, 1998, 2006, 2327, 1997, 2673, 2842, 1010, 2134, 1005, 1056, 2191, 2033, 2215, 2000, 2689, 1996, 3149, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í∏∞Îä• ÌôïÏù∏ : {'input_ids': [101, 2651, 2003, 6928, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ : {dataset['train'][0]['review']}\")\n",
    "print(f\"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Î≥ÄÌôòÎêú Îç∞Ïù¥ÌÑ∞ÏÖã : {tokenizer(dataset['train'][0]['review'])}\")\n",
    "print(f\"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í∏∞Îä• ÌôïÏù∏ : {tokenizer('Today is monday')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [00:05<00:00, 6193.96 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [00:02<00:00, 6379.89 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(\n",
    "        batch['review'], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=300)\n",
    "    return temp\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\",\n",
       " 'sentiment': 'negative',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  6659,\n",
       "  1012,\n",
       "  1045,\n",
       "  3685,\n",
       "  2131,\n",
       "  2058,\n",
       "  1996,\n",
       "  3143,\n",
       "  5949,\n",
       "  1997,\n",
       "  2307,\n",
       "  5848,\n",
       "  2023,\n",
       "  2265,\n",
       "  3397,\n",
       "  1012,\n",
       "  2023,\n",
       "  2003,\n",
       "  2025,\n",
       "  14036,\n",
       "  24584,\n",
       "  2389,\n",
       "  3772,\n",
       "  1010,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2074,\n",
       "  1037,\n",
       "  10036,\n",
       "  3535,\n",
       "  2000,\n",
       "  5466,\n",
       "  2619,\n",
       "  3297,\n",
       "  21699,\n",
       "  5889,\n",
       "  3031,\n",
       "  1037,\n",
       "  2754,\n",
       "  1998,\n",
       "  2031,\n",
       "  2068,\n",
       "  4685,\n",
       "  1037,\n",
       "  9996,\n",
       "  5301,\n",
       "  3496,\n",
       "  1012,\n",
       "  1045,\n",
       "  2031,\n",
       "  2941,\n",
       "  2589,\n",
       "  17727,\n",
       "  12298,\n",
       "  2147,\n",
       "  2004,\n",
       "  2019,\n",
       "  3364,\n",
       "  1010,\n",
       "  1998,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  2025,\n",
       "  17727,\n",
       "  12298,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  1996,\n",
       "  4378,\n",
       "  2003,\n",
       "  2941,\n",
       "  5870,\n",
       "  2012,\n",
       "  1006,\n",
       "  2065,\n",
       "  2027,\n",
       "  1005,\n",
       "  2128,\n",
       "  2941,\n",
       "  5870,\n",
       "  2012,\n",
       "  2023,\n",
       "  2265,\n",
       "  2012,\n",
       "  2035,\n",
       "  1010,\n",
       "  2009,\n",
       "  3504,\n",
       "  3243,\n",
       "  8275,\n",
       "  1007,\n",
       "  2003,\n",
       "  1996,\n",
       "  14325,\n",
       "  1997,\n",
       "  1996,\n",
       "  4113,\n",
       "  2732,\n",
       "  2108,\n",
       "  2439,\n",
       "  2066,\n",
       "  1037,\n",
       "  8448,\n",
       "  1999,\n",
       "  18167,\n",
       "  1012,\n",
       "  1996,\n",
       "  12873,\n",
       "  1010,\n",
       "  3294,\n",
       "  15142,\n",
       "  2477,\n",
       "  2027,\n",
       "  2272,\n",
       "  2039,\n",
       "  2007,\n",
       "  2024,\n",
       "  2054,\n",
       "  2111,\n",
       "  4756,\n",
       "  2012,\n",
       "  1012,\n",
       "  1998,\n",
       "  2065,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2112,\n",
       "  1997,\n",
       "  1996,\n",
       "  3496,\n",
       "  1010,\n",
       "  1996,\n",
       "  5889,\n",
       "  2097,\n",
       "  2425,\n",
       "  2068,\n",
       "  2008,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  3308,\n",
       "  999,\n",
       "  1045,\n",
       "  2424,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  29591,\n",
       "  2000,\n",
       "  1996,\n",
       "  2396,\n",
       "  1010,\n",
       "  1998,\n",
       "  3084,\n",
       "  2033,\n",
       "  5390,\n",
       "  2005,\n",
       "  3065,\n",
       "  2066,\n",
       "  3005,\n",
       "  2240,\n",
       "  2003,\n",
       "  2009,\n",
       "  4312,\n",
       "  1010,\n",
       "  2029,\n",
       "  2018,\n",
       "  2307,\n",
       "  5848,\n",
       "  1010,\n",
       "  2307,\n",
       "  17727,\n",
       "  12298,\n",
       "  2399,\n",
       "  1010,\n",
       "  1998,\n",
       "  2006,\n",
       "  2327,\n",
       "  1997,\n",
       "  2673,\n",
       "  2842,\n",
       "  1010,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  2191,\n",
       "  2033,\n",
       "  2215,\n",
       "  2000,\n",
       "  2689,\n",
       "  1996,\n",
       "  3149,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1) # ÏòàÏ∏°Í∞íÍ≥º Ïã§Ï†ú Î†àÏù¥Î∏î Îç∞Ïù¥ÌÑ∞Î•º ÌäúÌîåÎ°ú ÏûÖÎ†•\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµÏùÑ ÏúÑÌïú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ÏôÄ ÏÑ§Ï†ï Ï†ïÏùò\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # ÌïôÏäµ Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† ÎîîÎ†âÌÑ∞Î¶¨\n",
    "    overwrite_output_dir=True,            # Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨Ïóê Ïù¥ÎØ∏ ÏûàÎäî ÌååÏùºÏùÑ ÎçÆÏñ¥Ïì∏ÏßÄ Ïó¨Î∂Ä\n",
    "    num_train_epochs=3,                   # ÌïôÏäµÌï† ÏóêÌè¨ÌÅ¨(epoch) Ïàò\n",
    "    learning_rate=2e-5,                   # ÌïôÏäµÎ•† (learning rate)\n",
    "    per_device_train_batch_size=32,       # Í∞Å ÎîîÎ∞îÏù¥Ïä§(Ïòà: GPU)Îãπ ÌïôÏäµ Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    per_device_eval_batch_size=32,        # Í∞Å ÎîîÎ∞îÏù¥Ïä§Îãπ ÌèâÍ∞Ä Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    evaluation_strategy='epoch'           # ÌèâÍ∞Ä Ï†ÑÎûµ (Ïó¨Í∏∞ÏÑúÎäî Îß§ ÏóêÌè¨ÌÅ¨ÎßàÎã§ ÌèâÍ∞Ä)\n",
    ")\n",
    "\n",
    "# Trainer Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÏó¨ ÌïôÏäµ Î∞è ÌèâÍ∞ÄÎ•º Í¥ÄÎ¶¨\n",
    "trainer = Trainer(\n",
    "    model=model,                          # ÌïôÏäµÌï† Î™®Îç∏\n",
    "    args=args,                            # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "    train_dataset=dataset['train'],       # ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=dataset['test'],         # ÌèâÍ∞ÄÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,      # ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í≥ÑÏÇ∞ÌïòÎäî Ìï®Ïàò\n",
    "    tokenizer=tokenizer                   # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä (ÌÖçÏä§Ìä∏Î•º ÌÜ†ÌÅ∞ÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÎèÑÍµ¨)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 500/3282 [02:23<13:07,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4592, 'grad_norm': 8.490633964538574, 'learning_rate': 1.695307739183425e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 1000/3282 [04:45<10:53,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3554, 'grad_norm': 5.520948886871338, 'learning_rate': 1.3906154783668494e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1094/3282 [05:55<10:10,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3265502154827118, 'eval_accuracy': 0.8609333333333333, 'eval_runtime': 42.7699, 'eval_samples_per_second': 350.714, 'eval_steps_per_second': 10.966, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3282 [07:50<08:22,  3.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.302, 'grad_norm': 15.028241157531738, 'learning_rate': 1.0859232175502743e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2000/3282 [10:11<06:01,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.289, 'grad_norm': 21.859106063842773, 'learning_rate': 7.81230956733699e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3282 [11:47<04:44,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.294048547744751, 'eval_accuracy': 0.8768666666666667, 'eval_runtime': 42.5027, 'eval_samples_per_second': 352.919, 'eval_steps_per_second': 11.035, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2500/3282 [13:16<03:42,  3.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2615, 'grad_norm': 7.353476047515869, 'learning_rate': 4.765386959171238e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3000/3282 [15:37<01:19,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'grad_norm': 9.855073928833008, 'learning_rate': 1.7184643510054846e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3282/3282 [17:40<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2930574119091034, 'eval_accuracy': 0.8802, 'eval_runtime': 42.8248, 'eval_samples_per_second': 350.264, 'eval_steps_per_second': 10.952, 'epoch': 3.0}\n",
      "{'train_runtime': 1060.0341, 'train_samples_per_second': 99.053, 'train_steps_per_second': 3.096, 'train_loss': 0.3149757664208002, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3282, training_loss=0.3149757664208002, metrics={'train_runtime': 1060.0341, 'train_samples_per_second': 99.053, 'train_steps_per_second': 3.096, 'total_flos': 882184338000000.0, 'train_loss': 0.3149757664208002, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPUÏùò Ï∞®Ïù¥\n",
    "\n",
    "# Í∞ÑÎã®Ìïú ÎçßÏÖà Î¨∏Ï†ú 100Í∞ú Î¨∏Ï†úÎ•º ÎàÑÍ∞Ä Îçî Îπ®Î¶¨ ÌíÄÍπåÏöî?\n",
    "# - ÎåÄÌïôÏÉù 1Î™Ö(CPU) vs Ï¥àÎî© 100Î™Ö(GPU)\n",
    "# - NVIDIAÍ∞Ä Ïôú ÎØ∏ÏπúÎìØÏù¥ Ïò¨ÎûêÏ£†? => GPUÎßåÎì§ ÏûñÏïÑÏöî. \n",
    "\n",
    "# GPUÎûë Îî•Îü¨Îãù(Îã®Ïàú ÌñâÎ†¨ Í≥ÑÏÇ∞)ÏùÄ Î≠î ÏÉÅÍ¥ÄÏù¥ÏßÄ?\n",
    "# - Í∞ÑÎã®Ìïú ÎçßÏÖà Î¨∏Ï†ú Ìë∏ÎäîÎç∞ ÎåÄÌïôÏÉù 1Î™Ö (ÏãúÍ∏â 100ÎßåÏõê) -> Ï¥àÎî©100Î™Ö(ÏãúÍ∏â1ÎßåÏõê)\n",
    "\n",
    "# Îã§Íµ¥Ïóî Ïû•ÏÇ¨ÏóÜÎã§. => Ìè¨Ìè¥Ïù¥ Ï°∏Îùº ÎßéÏúºÎ©¥ ÎêòÏöî. Ìè¨Ìè¥ 1Í∞ú Ïù∏Í≤É Î≥¥Îã§ 2Í∞ú => 3Í∞ú => 4Í∞ú // Ïï± 10Í∞ú(Í≥ÑÏÇ∞Í∏∞...) => ÏΩîÌÖå ÏïàÎ¥ÖÎãàÎã§.\n",
    "# ÌöåÏÇ¨ÏóêÏÑú Î©îÏùºÏù¥ Ïò¨ Îïå ÏΩîÌÖå(ÌïÑÌÑ∞) ÏóÜÏù¥ Î∞îÎ°ú Î©¥Ï†ëÏùÑ Î≥¥ÏûêÍ≥† Ìï¥Ïöî.\n",
    "# ÏΩîÌÖå 30Î∂Ñ ÎØ∏ÎßåÏúºÎ°ú Î®∏Î¶¨ ÏãùÌûàÎäî Ïö©ÏúºÎ°ú => ÎÇòÎ®∏ÏßÄ ÏßÑÏßú Í≥µÎ∂ÄÎ•º ÌïòÏÑ∏Ïöî.\n",
    "# DE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt => pip install # 11Ïãú\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 469/469 [00:43<00:00, 10.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2930574119091034,\n",
       " 'eval_accuracy': 0.8802,\n",
       " 'eval_runtime': 43.4409,\n",
       " 'eval_samples_per_second': 345.296,\n",
       " 'eval_steps_per_second': 10.796,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.975800096988678},\n",
       " {'label': 'negative', 'score': 0.9816442131996155},\n",
       " {'label': 'positive', 'score': 0.975800096988678}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Î™®Îç∏ ÏÇ¨Ïö© Ïòà 1\n",
    "data = [\n",
    "    \"good\",\n",
    "    \"bad\",\n",
    "    \"good\"\n",
    "]\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9817873239517212}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Î™®Îç∏ ÏÇ¨Ïö© Ïòà 2\n",
    "data=[\n",
    "    \"\"\"When are you guys going to fix all the issues?? Firstly, none of the reaction emojis are showing up. It's just a grey circle. When scrolling, it doesn't move freely. There's like a delay!! Very frustrating!!! Also, nearly every post is either from a \"suggested page\" or a \"sponsered page\". I hardly ever see anything from the pages that I actually follow or my friends pages. No wonder so many people are leaving FB üôÑüôÑ\"\"\"\n",
    "]\n",
    "\n",
    "classifier(data)\n",
    "\n",
    "# Ïòà : ÌîÑÎ°úÍ∑∏Îû® : Íµ¨Í∏Ä ÌîåÎ†àÏù¥ Ïä§ÌÜ†Ïñ¥ ÎßÅÌÅ¨Î•º ÎÑ£ÏúºÎ©¥ > Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤¥ ÌÅ¨Î°§ÎßÅ > Î∂ÄÏ†ïÏùò Í∞ïÎèÑÍ∞Ä 0.8 Ïù¥ÏÉÅÏù∏ Î¶¨Î∑∞Îßå ÌïÑÌÑ∞ Í±∏Îü¨ÏÑú Í≥†Í∞ùÏÇ¨ÏóêÍ≤å Í≥µÏú†\n",
    "# Î≥Ñ 1Í∞úÏóê ÎÑ§Í±∞Ìã∞Î∏å 0.8Ïù¥ÏÉÅ > slackÏúºÎ°ú ÏïåÎ¶ºÎ≥¥ÎÇ¥Í∏∞ > (ÎÇò)ÎåÄÏùë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ÏùÑ s3Ïóê ÏóÖÎ°úÎìú\n",
    "# 1. AWS Î°úÍ∑∏Ïù∏ Ìïú Îã§Ïùå > Î≤ÑÌÇ∑ ÏÉùÏÑ±\n",
    "# 2. boto3Î•º ÌôúÏö©Ìï¥ÏÑú ÏΩîÎìú Î≤†Ïù¥Ïä§ s3 ÏÉùÏÑ± Î∞è ÌååÏùº ÏóÖÎ°úÎìú\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS buckets ÏÉùÏÑ± Î∞è Î∞∞Ìè¨\n",
    "1. IAM Í≥ÑÏ†ï ÏÉùÏÑ± / EC2 - pemÌÇ§ ÏÉùÏÑ±\n",
    "2. AWS CLI Î™ÖÎ†πÏñ¥ ÏÑ§Ïπò\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "> aws version\n",
    "> aws configure\n",
    "```\n",
    "AWS Access Key ID [****************SK6E]: AKIA4SDNV3Y4IRQFSK6E\n",
    "AWS Secret Access Key [****************Eyxt]: 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\n",
    "Default region name [019mY+oLFtxNVxyMayCYZdSdbXeb2FwxWu5LaLxT]:  ap-northeast-2\n",
    "Default output format [json]: json\n",
    "```\n",
    "> aws confiure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aws cli ÏÑ§Ïπò\n",
    "- curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\n",
    "- sudo installer -pkg AWSCLIV2.pkg -target /\n",
    "\n",
    "#### ÏÑ§Ïπò ÌôïÏù∏\n",
    "- which aws\n",
    "/usr/local/bin/aws \n",
    "- aws --version\n",
    "aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3Î•º ÌÜµÌï¥ AWS s3 bucketÏùÑ ÌôúÏö©Ìï¥ Î≤ÑÌÇ∑ ÏÉùÏÑ±\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3 = boto3. client('s3') # s3 ÏΩòÏÜîÏóê Ï†ëÏÜç\n",
    "bucket_name = 'nam_models_bucket'\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    response = s3.list_buckets()\n",
    "\n",
    "    bucket_list = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        bucket_list.append(bucket_list[\"Name\"])\n",
    "\n",
    "    if bucket_name not in bucket_list:\n",
    "        try:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint':'ap-northeast-2'}\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            print('Ïò§Î•ò Î∞úÏÉù :', e)\n",
    "\n",
    "            if e.response['Error']['Code'] == 'BucketAlreadyExists':    \n",
    "                print('Îã§Î•∏ Î≤ÑÌÇ∑ Ïù¥Î¶ÑÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.')\n",
    "\n",
    "            elif e.reponse['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "                print('Ïù¥ÎØ∏ ÎßåÎì§Ïñ¥ Ï†∏ÏûàÎäî Î≤ÑÌÇ∑ÏûÖÎãàÎã§.')\n",
    "                \n",
    "            else:\n",
    "                print('Î≤ÑÌÇ∑ ÎßåÎì§Í∏∞Î•º Ïû¨ÏãúÎèÑ Ï§ëÏûÖÎãàÎã§.')\n",
    "                time.sleep(3)\n",
    "                create_bucket(bucket_name)\n",
    "\n",
    "# aws credentials\n",
    "# aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3Ïóê ÌååÏùº ÏóÖÎ°úÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = '112test-bucket'\n",
    "file_path = 'tinybert-sentiment-analysis'\n",
    "\n",
    "# s3.upload_file() # Ïò§ÏßÅ ÌååÏùºÎßå (Ìè¥Îçî ÏïàÎê®))\n",
    "\n",
    "# Ìè¥Îçî ÏïàÏóê ÏûàÎäî Î™®Îì† ÌååÏùº ÏóÖÎ°úÎìú\n",
    "def s3_upload_file_folder_name(model_folder, folder_name):\n",
    "    for root, dir, files in os.walk(model_folder):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            s3_key = os.path.join(folder_name, file_name)\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "\n",
    "s3_upload_file_folder_name(file_path, 'tinybert-test-folder')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
