{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리뷰 긍정 부정 판단 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 준비\n",
    "- 가상환경 셋팅\n",
    "- 필요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터로드 \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:00<00:00, 46115.19 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:00<00:00, 49694.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': [\"This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\",\n",
       "  \"What a horrible movie. This movie was so out of order and so hard to follow.It was so hard to follow and was just confusing. The whole time I was watching it I was wishing it would end!!I felt like I wasted 2hours of my life that I will never get back. Save your money and don't rent this movie. I now see why Sarah Michelle Gellar was barely in the movie. The first movie was great but this was just sucked. I would never recommend this movie to anyone. Save your money and watch the trailer because that is about the only thing that is worth seeing with this movie. This movie had no real story to it either. I am still wondering what I watched.\",\n",
       "  'Sorry did i miss something? did i walk out early? The first ten minutes of unusual (and untrue!) stories had me thinking \"This is going to be a classic\" But it was all down hill from there! The acting was brilliant, for what it\\'s worth William H Macy is fantastic and just gets better and better every film i watch him in. But it never seemed to connect. I was waiting for the big moment where all the stories inter connect and then suddenly..it rains frog?? it was if the writer said \"i\\'ve gone to deep how can i pull all these stories together cleverely....Oh sod it i\\'ll just have it raining frogs\". I like clever movies, i like strange movies but this was just odd and boring. 4/10',\n",
       "  \"I loved this mini series. Tara Fitzgerald did an incredible job portraying Helen Graham, a beautiful young woman hiding, along with her young son, from a mysterious past. As an anglophile who loves romances... this movie was just my cup of tea and I would recommend it to anyone looking to escape for a few hours into the England of the 1800's. I also must mention that Toby Stephens who portrays the very magnetic Gilbert Markham is reason enough to watch this wonderful production.\",\n",
       "  'I was excited when I heard they were finally making this horrific event into a movie. The whole era (1980\\'s Southern California) and subject matter (drug and porn industry) is intriguing to me. I thought this would be a sure fire hit. I was not thrilled with the choice of Kilmer as Holmes, they do not resemble each other in physical appearance or mannerisms. I guess he sells tickets? However, I was willing to overlook this and give it a fair shot. I was a bit shocked that there were only like four other people in the entire theater with me on that first day of showing. Now the whole crime and story in the film is hard to do, I will admit that. There were no witnesses to this very violent and brutal act. John Holmes was there, but he was also a pathological liar and worried about what would happen to his family (and self) if he talked to police about it. In fact, Holmes never really testified about what happened and the crime did go unsolved. So this was still really one big mystery, a mystery that this movie does nothing to cast light on. The person writing the screenplay had a whole lot of discretion and most of the principal characters are dead. However, there is no real storyline, it is fragmented claptrap. The script is light and the actors try to hard to beef up paper thin lines by overacting. The film gives no insight into Holmes or the other people involved. Kilmer\\'s character disappears for long stretches, his girlfriend is dull, the police are jokes. Even Kudrow tries hard to make a flimsy role look substantial. It is a very shallow piece and dare I say, boring. The director even tries to turn it into a love story. Which is nice, unless you know anything about what a piece of trash John Holmes really was. Perhaps a couple of viewings of Anderson\\'s \"Boogie Nights\" might have helped here. \"Boogie Nights\" was innovative and exciting in all regards. This film on the other hand was flat and without any real charm or style. Even the music is out of place, with Duran Duran being played in a scene that was supposed to have taken place in 1980. Then we have Gordon Lightfoot? Gordon Lightfoot? There could have been a great film based on this gruesome event, but I have not seen it yet. I have not seen even a decent one yet (unless you consider the Rahad Jackson scene from Boogie Nights).'],\n",
       " 'sentiment': ['negative', 'negative', 'negative', 'positive', 'negative'],\n",
       " 'label': [0, 0, 0, 1, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hugging Face에 저장되어 있는 모델을 불러올게요. TinyBERT 모델을 불러오겠습니다.\n",
    "model = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋의 데이터 : This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\n",
      "토크나이저로 변환된 데이터셋 : {'input_ids': [101, 2023, 2265, 2003, 6659, 1012, 1045, 3685, 2131, 2058, 1996, 3143, 5949, 1997, 2307, 5848, 2023, 2265, 3397, 1012, 2023, 2003, 2025, 14036, 24584, 2389, 3772, 1010, 2009, 1005, 1055, 2074, 1037, 10036, 3535, 2000, 5466, 2619, 3297, 21699, 5889, 3031, 1037, 2754, 1998, 2031, 2068, 4685, 1037, 9996, 5301, 3496, 1012, 1045, 2031, 2941, 2589, 17727, 12298, 2147, 2004, 2019, 3364, 1010, 1998, 2023, 2265, 2003, 2025, 17727, 12298, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 1996, 4378, 2003, 2941, 5870, 2012, 1006, 2065, 2027, 1005, 2128, 2941, 5870, 2012, 2023, 2265, 2012, 2035, 1010, 2009, 3504, 3243, 8275, 1007, 2003, 1996, 14325, 1997, 1996, 4113, 2732, 2108, 2439, 2066, 1037, 8448, 1999, 18167, 1012, 1996, 12873, 1010, 3294, 15142, 2477, 2027, 2272, 2039, 2007, 2024, 2054, 2111, 4756, 2012, 1012, 1998, 2065, 2009, 1005, 1055, 2025, 2112, 1997, 1996, 3496, 1010, 1996, 5889, 2097, 2425, 2068, 2008, 2009, 1005, 1055, 3308, 999, 1045, 2424, 2023, 2265, 2003, 29591, 2000, 1996, 2396, 1010, 1998, 3084, 2033, 5390, 2005, 3065, 2066, 3005, 2240, 2003, 2009, 4312, 1010, 2029, 2018, 2307, 5848, 1010, 2307, 17727, 12298, 2399, 1010, 1998, 2006, 2327, 1997, 2673, 2842, 1010, 2134, 1005, 1056, 2191, 2033, 2215, 2000, 2689, 1996, 3149, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "토크나이저 기능 확인 : {'input_ids': [101, 2651, 2003, 6928, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"데이터셋의 데이터 : {dataset['train'][0]['review']}\")\n",
    "print(f\"토크나이저로 변환된 데이터셋 : {tokenizer(dataset['train'][0]['review'])}\")\n",
    "print(f\"토크나이저 기능 확인 : {tokenizer('Today is monday')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:05<00:00, 6193.96 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:02<00:00, 6379.89 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(\n",
    "        batch['review'], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=300)\n",
    "    return temp\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This show is terrible. I cannot get over the complete waste of great talent this show contains. This is not entertaining improvisational acting, it's just a cheap attempt to throw someone famous comedic actors onto a stage and have them perform a poorly improved scene. I have actually done improv work as an actor, and this show is not improv.<br /><br />What the audience is actually laughing at (if they're actually laughing at this show at all, it looks quite fake) is the embarrassment of the guest star being lost like a deer in headlights. The dumb, completely unrelated things they come up with are what people laugh at. And if it's not part of the scene, the actors will tell them that it's wrong! I find this show is disgrace to the art, and makes me cry for shows like Whose Line is it Anyway, which had great talent, great improv games, and on top of everything else, didn't make me want to change the channel.\",\n",
       " 'sentiment': 'negative',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  6659,\n",
       "  1012,\n",
       "  1045,\n",
       "  3685,\n",
       "  2131,\n",
       "  2058,\n",
       "  1996,\n",
       "  3143,\n",
       "  5949,\n",
       "  1997,\n",
       "  2307,\n",
       "  5848,\n",
       "  2023,\n",
       "  2265,\n",
       "  3397,\n",
       "  1012,\n",
       "  2023,\n",
       "  2003,\n",
       "  2025,\n",
       "  14036,\n",
       "  24584,\n",
       "  2389,\n",
       "  3772,\n",
       "  1010,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2074,\n",
       "  1037,\n",
       "  10036,\n",
       "  3535,\n",
       "  2000,\n",
       "  5466,\n",
       "  2619,\n",
       "  3297,\n",
       "  21699,\n",
       "  5889,\n",
       "  3031,\n",
       "  1037,\n",
       "  2754,\n",
       "  1998,\n",
       "  2031,\n",
       "  2068,\n",
       "  4685,\n",
       "  1037,\n",
       "  9996,\n",
       "  5301,\n",
       "  3496,\n",
       "  1012,\n",
       "  1045,\n",
       "  2031,\n",
       "  2941,\n",
       "  2589,\n",
       "  17727,\n",
       "  12298,\n",
       "  2147,\n",
       "  2004,\n",
       "  2019,\n",
       "  3364,\n",
       "  1010,\n",
       "  1998,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  2025,\n",
       "  17727,\n",
       "  12298,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  1996,\n",
       "  4378,\n",
       "  2003,\n",
       "  2941,\n",
       "  5870,\n",
       "  2012,\n",
       "  1006,\n",
       "  2065,\n",
       "  2027,\n",
       "  1005,\n",
       "  2128,\n",
       "  2941,\n",
       "  5870,\n",
       "  2012,\n",
       "  2023,\n",
       "  2265,\n",
       "  2012,\n",
       "  2035,\n",
       "  1010,\n",
       "  2009,\n",
       "  3504,\n",
       "  3243,\n",
       "  8275,\n",
       "  1007,\n",
       "  2003,\n",
       "  1996,\n",
       "  14325,\n",
       "  1997,\n",
       "  1996,\n",
       "  4113,\n",
       "  2732,\n",
       "  2108,\n",
       "  2439,\n",
       "  2066,\n",
       "  1037,\n",
       "  8448,\n",
       "  1999,\n",
       "  18167,\n",
       "  1012,\n",
       "  1996,\n",
       "  12873,\n",
       "  1010,\n",
       "  3294,\n",
       "  15142,\n",
       "  2477,\n",
       "  2027,\n",
       "  2272,\n",
       "  2039,\n",
       "  2007,\n",
       "  2024,\n",
       "  2054,\n",
       "  2111,\n",
       "  4756,\n",
       "  2012,\n",
       "  1012,\n",
       "  1998,\n",
       "  2065,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2112,\n",
       "  1997,\n",
       "  1996,\n",
       "  3496,\n",
       "  1010,\n",
       "  1996,\n",
       "  5889,\n",
       "  2097,\n",
       "  2425,\n",
       "  2068,\n",
       "  2008,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  3308,\n",
       "  999,\n",
       "  1045,\n",
       "  2424,\n",
       "  2023,\n",
       "  2265,\n",
       "  2003,\n",
       "  29591,\n",
       "  2000,\n",
       "  1996,\n",
       "  2396,\n",
       "  1010,\n",
       "  1998,\n",
       "  3084,\n",
       "  2033,\n",
       "  5390,\n",
       "  2005,\n",
       "  3065,\n",
       "  2066,\n",
       "  3005,\n",
       "  2240,\n",
       "  2003,\n",
       "  2009,\n",
       "  4312,\n",
       "  1010,\n",
       "  2029,\n",
       "  2018,\n",
       "  2307,\n",
       "  5848,\n",
       "  1010,\n",
       "  2307,\n",
       "  17727,\n",
       "  12298,\n",
       "  2399,\n",
       "  1010,\n",
       "  1998,\n",
       "  2006,\n",
       "  2327,\n",
       "  1997,\n",
       "  2673,\n",
       "  2842,\n",
       "  1010,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  2191,\n",
       "  2033,\n",
       "  2215,\n",
       "  2000,\n",
       "  2689,\n",
       "  1996,\n",
       "  3149,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1) # 예측값과 실제 레이블 데이터를 튜플로 입력\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yeongnam/Desktop/yeardream/05_MLops/MLOps_02/fastapi-backend-bertmodel/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 모델 학습을 위한 하이퍼파라미터와 설정 정의\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # 학습 결과를 저장할 디렉터리\n",
    "    overwrite_output_dir=True,            # 출력 디렉터리에 이미 있는 파일을 덮어쓸지 여부\n",
    "    num_train_epochs=3,                   # 학습할 에포크(epoch) 수\n",
    "    learning_rate=2e-5,                   # 학습률 (learning rate)\n",
    "    per_device_train_batch_size=32,       # 각 디바이스(예: GPU)당 학습 배치 크기\n",
    "    per_device_eval_batch_size=32,        # 각 디바이스당 평가 배치 크기\n",
    "    evaluation_strategy='epoch'           # 평가 전략 (여기서는 매 에포크마다 평가)\n",
    ")\n",
    "\n",
    "# Trainer 객체를 생성하여 학습 및 평가를 관리\n",
    "trainer = Trainer(\n",
    "    model=model,                          # 학습할 모델\n",
    "    args=args,                            # 학습 파라미터 설정\n",
    "    train_dataset=dataset['train'],       # 학습에 사용할 데이터셋\n",
    "    eval_dataset=dataset['test'],         # 평가에 사용할 데이터셋\n",
    "    compute_metrics=compute_metrics,      # 평가 지표를 계산하는 함수\n",
    "    tokenizer=tokenizer                   # 토크나이저 (텍스트를 토큰으로 변환하는 도구)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 500/3282 [02:23<13:07,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4592, 'grad_norm': 8.490633964538574, 'learning_rate': 1.695307739183425e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1000/3282 [04:45<10:53,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3554, 'grad_norm': 5.520948886871338, 'learning_rate': 1.3906154783668494e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 1094/3282 [05:55<10:10,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3265502154827118, 'eval_accuracy': 0.8609333333333333, 'eval_runtime': 42.7699, 'eval_samples_per_second': 350.714, 'eval_steps_per_second': 10.966, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1500/3282 [07:50<08:22,  3.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.302, 'grad_norm': 15.028241157531738, 'learning_rate': 1.0859232175502743e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 2000/3282 [10:11<06:01,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.289, 'grad_norm': 21.859106063842773, 'learning_rate': 7.81230956733699e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 2188/3282 [11:47<04:44,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.294048547744751, 'eval_accuracy': 0.8768666666666667, 'eval_runtime': 42.5027, 'eval_samples_per_second': 352.919, 'eval_steps_per_second': 11.035, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 2500/3282 [13:16<03:42,  3.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2615, 'grad_norm': 7.353476047515869, 'learning_rate': 4.765386959171238e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 3000/3282 [15:37<01:19,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'grad_norm': 9.855073928833008, 'learning_rate': 1.7184643510054846e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 3282/3282 [17:40<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2930574119091034, 'eval_accuracy': 0.8802, 'eval_runtime': 42.8248, 'eval_samples_per_second': 350.264, 'eval_steps_per_second': 10.952, 'epoch': 3.0}\n",
      "{'train_runtime': 1060.0341, 'train_samples_per_second': 99.053, 'train_steps_per_second': 3.096, 'train_loss': 0.3149757664208002, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3282, training_loss=0.3149757664208002, metrics={'train_runtime': 1060.0341, 'train_samples_per_second': 99.053, 'train_steps_per_second': 3.096, 'total_flos': 882184338000000.0, 'train_loss': 0.3149757664208002, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU의 차이\n",
    "\n",
    "# 간단한 덧셈 문제 100개 문제를 누가 더 빨리 풀까요?\n",
    "# - 대학생 1명(CPU) vs 초딩 100명(GPU)\n",
    "# - NVIDIA가 왜 미친듯이 올랐죠? => GPU만들 잖아요. \n",
    "\n",
    "# GPU랑 딥러닝(단순 행렬 계산)은 뭔 상관이지?\n",
    "# - 간단한 덧셈 문제 푸는데 대학생 1명 (시급 100만원) -> 초딩100명(시급1만원)\n",
    "\n",
    "# 다굴엔 장사없다. => 포폴이 졸라 많으면 되요. 포폴 1개 인것 보다 2개 => 3개 => 4개 // 앱 10개(계산기...) => 코테 안봅니다.\n",
    "# 회사에서 메일이 올 때 코테(필터) 없이 바로 면접을 보자고 해요.\n",
    "# 코테 30분 미만으로 머리 식히는 용으로 => 나머지 진짜 공부를 하세요.\n",
    "# DE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt => pip install # 11시\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:43<00:00, 10.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2930574119091034,\n",
       " 'eval_accuracy': 0.8802,\n",
       " 'eval_runtime': 43.4409,\n",
       " 'eval_samples_per_second': 345.296,\n",
       " 'eval_steps_per_second': 10.796,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.975800096988678},\n",
       " {'label': 'negative', 'score': 0.9816442131996155},\n",
       " {'label': 'positive', 'score': 0.975800096988678}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 예 1\n",
    "data = [\n",
    "    \"good\",\n",
    "    \"bad\",\n",
    "    \"good\"\n",
    "]\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9817873239517212}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 예 2\n",
    "data=[\n",
    "    \"\"\"When are you guys going to fix all the issues?? Firstly, none of the reaction emojis are showing up. It's just a grey circle. When scrolling, it doesn't move freely. There's like a delay!! Very frustrating!!! Also, nearly every post is either from a \"suggested page\" or a \"sponsered page\". I hardly ever see anything from the pages that I actually follow or my friends pages. No wonder so many people are leaving FB 🙄🙄\"\"\"\n",
    "]\n",
    "\n",
    "classifier(data)\n",
    "\n",
    "# 예 : 프로그램 : 구글 플레이 스토어 링크를 넣으면 > 리뷰 데이터 전체 크롤링 > 부정의 강도가 0.8 이상인 리뷰만 필터 걸러서 고객사에게 공유\n",
    "# 별 1개에 네거티브 0.8이상 > slack으로 알림보내기 > (나)대응"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 s3에 업로드\n",
    "# 1. AWS 로그인 한 다음 > 버킷 생성\n",
    "# 2. boto3를 활용해서 코드 베이스 s3 생성 및 파일 업로드\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS buckets 생성 및 배포\n",
    "1. IAM 계정 생성 / EC2 - pem키 생성\n",
    "2. AWS CLI 명령어 설치\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "> aws version\n",
    "> aws configure\n",
    "```\n",
    "AWS Access Key ID [****************SK6E]: AKIA4SDNV3Y4IRQFSK6E\n",
    "AWS Secret Access Key [****************Eyxt]: 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\n",
    "Default region name [019mY+oLFtxNVxyMayCYZdSdbXeb2FwxWu5LaLxT]:  ap-northeast-2\n",
    "Default output format [json]: json\n",
    "```\n",
    "> aws confiure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aws cli 설치\n",
    "- curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\n",
    "- sudo installer -pkg AWSCLIV2.pkg -target /\n",
    "\n",
    "#### 설치 확인\n",
    "- which aws\n",
    "/usr/local/bin/aws \n",
    "- aws --version\n",
    "aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3를 통해 AWS s3 bucket을 활용해 버킷 생성\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3 = boto3. client('s3') # s3 콘솔에 접속\n",
    "bucket_name = 'nam_models_bucket'\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    response = s3.list_buckets()\n",
    "\n",
    "    bucket_list = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        bucket_list.append(bucket_list[\"Name\"])\n",
    "\n",
    "    if bucket_name not in bucket_list:\n",
    "        try:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint':'ap-northeast-2'}\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            print('오류 발생 :', e)\n",
    "\n",
    "            if e.response['Error']['Code'] == 'BucketAlreadyExists':    \n",
    "                print('다른 버킷 이름을 입력하세요.')\n",
    "\n",
    "            elif e.reponse['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "                print('이미 만들어 져있는 버킷입니다.')\n",
    "                \n",
    "            else:\n",
    "                print('버킷 만들기를 재시도 중입니다.')\n",
    "                time.sleep(3)\n",
    "                create_bucket(bucket_name)\n",
    "\n",
    "# aws credentials\n",
    "# aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3에 파일 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = '112test-bucket'\n",
    "file_path = 'tinybert-sentiment-analysis'\n",
    "\n",
    "# s3.upload_file() # 오직 파일만 (폴더 안됨))\n",
    "\n",
    "# 폴더 안에 있는 모든 파일 업로드\n",
    "def s3_upload_file_folder_name(model_folder, folder_name):\n",
    "    for root, dir, files in os.walk(model_folder):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            s3_key = os.path.join(folder_name, file_name)\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "\n",
    "s3_upload_file_folder_name(file_path, 'tinybert-test-folder')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
