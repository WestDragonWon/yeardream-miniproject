# 스파크 잡

## 개요

스파크 잡을 쿠버네티스에서 관리되도록 제출할 수 있습니다. (자세한 방법은 [이전 디렉토리 README.md](../README.md)의 실행 방법에 작성해놓았습니다.)

다양한 수집부에 의해 Kafka 에 들어오는 데이터를 S3에 저장하는 Streaming job을 만들었습니다. (./jobs 디렉토리)


### 1. 카프카 메시지 스키마 정의
메시지에 대한 처리를 하려면 데이터 스키마가 정의되어있어야 합니다.  
각 속성의 이름과, 타입을 명시해줍니다.

### 2. 스파크 세션 생성
스파크를 사용하여 Job을 돌리기위한 세션을 생성합니다.    
driver의 이름이 될 appname과, 필요한 종속성을 명시해줍니다.

### 3. 카프카의 데이터를 구독
다양한 원천 데이터는 전부 카프카를 통해 S3에 저장되도록 하였습니다.  
Spark의 readstream을 사용하여 카프카 특정 토픽을 구독합니다.
  
### 4. 처리 (Transform)
데이터가 저장되기 전에 이상 거래를 탐지하거나, 암호화를 하는 등의 작업을 할 수 있습니다. 이상치를 원천 데이터 저장소에 저장하고 싶지 않은 경우, 이 과정에서 제외할 수 있습니다.

### 5. 저장  
writeStream 형식으로 S3에 데이터를 저장합니다.  
분석을 편하게 하기 위해 연/월/일/시 디렉토리 구조로 데이터를 저장하도록 하였습니다.